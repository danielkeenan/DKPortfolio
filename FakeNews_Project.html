<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
    <head>
        <title>Daniel Keenan | Data Analyst</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <link rel="stylesheet" href="assets/css/main.css" />
        <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
    </head>

    <body class="is-preload">
		
		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
				<header id="header" class="alt">
					<h1>Fake News Data Analysis</h1>
					<h2>Exploring Fake and Real News Datasets</h2>
					<p>In this project, I have performed some exploratory analysis into fake and real
                        news datasets and performed feature selection and logistic regression to
                         determine how well the model can perform.</p>
                         <p>October 2021</p>
				</header>

			<!-- Main -->
				<div id="main">

					<!-- Content -->
						<section id="content" class="main">
							<h3>Import statements:</h3>
        <pre><code>        import numpy as np
        import pandas as pd
        from sklearn.feature_extraction.text import CountVectorizer
        from nltk.tokenize import RegexpTokenizer
        import plotly.graph_objects as go
        from sklearn.model_selection import train_test_split
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import accuracy_score, f1_score, log_loss
        import seaborn as sns
        import matplotlib.pyplot as plt
        from sklearn.metrics import plot_confusion_matrix
        from sklearn.metrics import classification_report
        from wordcloud import WordCloud</code></pre>
							
							<h3>Reading in the datasets:</h3>
        <pre><code>        fake_df = pd.read_csv('Fake.csv')
        fake_df.dropna(inplace=True)
        fake_df.head()</code></pre>

        <img src = 'images/FakeNews_Project/readcsv_fake.png'>
        <p></p>

        <pre><code>        real_df = pd.read_csv('True.csv')
        real_df.dropna(inplace=True)
        real_df.head()</code></pre>


        <img src = 'images/FakeNews_Project/readcsv_real.png'>
        <p></p>

        <h3>Concatenating the two datasets:</h3>
        <pre><code>        fake_df['class'] = np.zeros(fake_df.shape[0])
        real_df['class'] = np.ones(real_df.shape[0])
        
        news = pd.concat([fake_df, real_df])</code></pre>
        <img src = 'images/FakeNews_Project/concat.png'>
        <p></p>
    
        <h3>Combine the title and text columns:</h3>
        <pre><code>        news['title_text'] = news['title'] + ' ' +news['text']
        news.drop(['title', 'text'], axis=1, inplace=True)</code></pre>
        <img src = 'images/FakeNews_Project/combined_columns.png'>
        <p></p>
    
        <h3>Visualising the difference in fake and real news:</h3>
        <pre><code>        fig = go.Figure(data = [
        go.Pie(labels=['Fake','True'], values = list(news['class'].value_counts()),
                hole=0.3)
        ])
        
        fig.update_layout(title='Fake and Real News Ratio')
        fig.show()</code></pre>
        <img src = 'images/FakeNews_Project/pie.png'>
        <p></p>

        <h3>CountVectorizer:</h3>
        <pre><code>        token = RegexpTokenizer(r'[a-zA-Z]+')

        cv = CountVectorizer(lowercase=True, 
                                stop_words='english',
                                ngram_range = (1,2), 
                                tokenizer = token.tokenize, 
                                max_features=5000)
        
        text_counts = cv.fit_transform(news['title_text'])
        text_counts</code></pre>
    
        <h3>Train Test Split:</h3>
        <pre><code>        X = text_counts
        y = news['class']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</code></pre>
    
        <h3>Fitting the Logistic Regression Model:</h3>
        <pre><code>        model = LogisticRegression(class_weight='balanced', penalty='l2', solver='liblinear',
             max_iter=1000)
        model.fit(X_train, y_train)</code></pre>
        
        <h3>Predicting the model on the training and testing sets:</h3>
        <pre><code>        y_pred = model.predict(X_train)

        acc = accuracy_score(y_train, y_pred)
        f1 = f1_score(y_train, y_pred)
        l1 = log_loss(y_train, y_pred)
        
        print(f'(train) accuracy_score: {acc})')
        print(f'(train) f1_score: {f1})')
        print(f'(train) log_loss: {l1})')</code></pre>

        <img src = 'images/FakeNews_Project/predict_train.png'>
        <p></p>

        <pre><code>        y_pred = model.predict(X_test)

        acc = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        l1 = log_loss(y_test, y_pred)
        
        print(f'(train) accuracy_score: {acc})')
        print(f'(train) f1_score: {f1})')
        print(f'(train) log_loss: {l1})')</code></pre>

        <img src = 'images/FakeNews_Project/predict_test.png'>
        <p></p>
    
        <h3>Confusion Matrix:</h3>
        <pre><code>        disp = plot_confusion_matrix(model, X, y, display_labels=['Fake','True'], normalize=None)
        plt.grid(False)
        plt.show()</code></pre>
        <img src = 'images/FakeNews_Project/confusion_matrix.png'>
        <p></p>
    
        <h3>Classification Report:</h3>
        <pre><code>        y_pred = model.predict(X)
        print(classification_report(y, y_pred, target_names=['Fake','True'], zero_division=0))</code></pre>
        <img src = 'images/FakeNews_Project/classification_report.png'>
        <p></p>

        <h3>Generating a WordCloud:</h3>
        <pre><code>        top_features = {}
        for col, coef in zip(cv.get_feature_names(), model.coef_[0]):
            if np.abs(coef) > 0.3:
                top_features[col] = int(np.abs(coef)*100)</code></pre>

        <pre><code>        x, y = np.ogrid[:300, :300]

        mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2
        mask = 255 * mask.astype(int)
        
        wc = WordCloud(background_color='white', repeat=True, mask=mask, scale=2)
        wc.generate_from_frequencies(top_features)
        
        plt.figure(figsize=(15,15))
        plt.axis('off')
        plt.imshow(wc, interpolation='bilinear')
        plt.show()</code></pre>
        <img src = 'images/FakeNews_Project/wordcloud.png'>
		<p></p>
						</section>

				</div>

			<!-- Footer -->
				<footer id="footer">
				</footer>

		</div>

        <!-- Scripts -->
            <script src="assets/js/jquery.min.js"></script>
            <script src="assets/js/jquery.scrollex.min.js"></script>
            <script src="assets/js/jquery.scrolly.min.js"></script>
            <script src="assets/js/browser.min.js"></script>
            <script src="assets/js/breakpoints.min.js"></script>
            <script src="assets/js/util.js"></script>
            <script src="assets/js/main.js"></script>

    </body>
</html>